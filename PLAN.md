# Ray Marching Algorithm Benchmark â€” Plan

> Last updated: 2026-02-08

## Current Status

We have a working single-strategy, single-scene CLI runner:
```
uv run -m raymarching_benchmark --width 64 --height 48
â†’ Strategy: Standard | Scene: Sphere â€” runs and produces stats.json + heatmap PNG
```

### What's Done (âœ…)

| Module | Status |
|--------|--------|
| Core math (`Vec3`, `Ray`, `Camera`) | Complete, working |
| Type system (`MarchResult`, `RayMarchStats`) | Complete (one bug â€” see below) |
| 14 SDF test scenes (smooth, sharp, thin, fractal, CSG, stress) | Complete |
| SDF primitives + combinators + transforms | Complete |
| All 7 strategy implementations | Complete (Standard, Overstep-Bisect, Adaptive Hybrid, Relaxed, Auto-Relaxed, Enhanced, Segment) |
| Per-ray metrics collection (`MetricsCollector`) | Working |
| Iteration heatmap PNG output | Working |
| `MetricsAnalyzer` (comparison logic) | Implemented |
| `print_comparison_tables()` | Implemented |
| `BenchmarkConfig` dataclass | Implemented |
| Scene `suggested_camera()` overrides | Implemented on all 14 scenes |
| Scene `known_lipschitz_bound()` | Implemented on all 14 scenes |

### What's Broken (ğŸ›)

1. **`types.py` â€” `compute()` never assigns `self.hit_map`.**
   `compute()` creates a local `hit_map` array but forgets `self.hit_map = hit_map`.
   â†’ Hit-map and depth-map output is silently skipped.

2. **`main.py` â€” `--strategy` flag is a no-op.**
   Strategy is hardcoded: `strategy = StandardSphereTracing()`.
   No strategy registry/dispatch exists.

### What's Implemented but Dead Code (ğŸ”Œ)

- `MetricsAnalyzer` â€” never instantiated or called by CLI
- `print_comparison_tables()` â€” never called
- `BenchmarkConfig` â€” never used (CLI builds `RenderConfig`/`MarchConfig` directly)
- Scene `suggested_camera()` â€” never consumed by `main.py`
- Scene `known_lipschitz_bound()` â†’ `SegmentTracing.lipschitz` â€” never wired
- 6 of 7 strategies (everything except Standard)
- `visualization/charts.py` â€” stub only

### What's Missing Entirely (âŒ)

- **Strategy dispatch** â€” no registry, no `--strategy all` support
- **Matrix mode** â€” run all strategies Ã— all scenes in one invocation
- **Comparative analysis output** â€” tables, rankings, per-scene best/worst
- **Charts** (`charts.py` is an empty stub)
- **Tests** â€” `tests/` directory is empty
- **`--json` multi-scene bug** â€” overwrites file each iteration when multiple scenes given
- **Warp divergence** in output (metric is computed but not surfaced)

---

## Phase 1 â€” Bug Fixes & Wiring (get existing code working)

1. **Fix `hit_map` bug in `types.py`** â€” add `self.hit_map = hit_map` in `compute()`.
2. **Build strategy registry** â€” add `get_strategy_by_name(name)` in `strategies/__init__.py` and wire `--strategy` flag in `main.py`.
3. **Wire scene `suggested_camera()`** â€” use scene's camera defaults when the user doesn't override.
4. **Wire `known_lipschitz_bound()`** â†’ pass to `SegmentTracing` automatically.
5. **Fix `--json` multi-scene overwrite** â€” accumulate results, write once at end.

## Phase 2 â€” Matrix Mode & Comparative Analysis

6. **Add `--strategy all` and `--scene all`** â€” support running the full matrix.
7. **Wire `MetricsAnalyzer`** â€” accumulate results across all runs in a session.
8. **Wire `print_comparison_tables()`** â€” display after matrix run completes.
9. **Surface warp divergence** â€” include in stats.json and comparison tables.
10. **Improve console output** â€” progress bar or at least per-run status during matrix runs.

## Phase 3 â€” Visualization & Reporting

11. **Implement `charts.py`** â€” bar charts (iterations by strategy per scene), scatter plots (accuracy vs speed), heatmap grids.
12. **Side-by-side heatmap comparison** â€” iteration heatmaps for all strategies on one scene, tiled.
13. **Generate summary report** â€” markdown or HTML with tables + embedded charts.
14. **Save comparative results** â€” single JSON and/or CSV with full matrix data.

## Phase 4 â€” Validation & Polish

15. **Add smoke tests** â€” `tests/test_smoke.py` covering Vec3, Camera, each strategy on a simple scene.
16. **Add strategy correctness tests** â€” verify all strategies agree on hit/miss for known rays.
17. **Validate `SegmentTracing` iteration counting** â€” it currently counts SDF evaluations, not march steps. Decide on convention and document.
18. **Scene name matching** â€” replace fragile substring matching with exact-name lookup (+ alias support).
19. **README** â€” usage examples, sample output, architecture diagram.

## Phase 5 â€” Stretch Goals

20. **Performance profiling** â€” identify bottleneck (likely pure-Python SDF eval), consider numpy vectorization.
21. **Taichi GPU backend** â€” optional GPU acceleration for large resolutions.
22. **Additional scenes** â€” user-contributed scenes, parametric scene generators.
23. **Interactive mode** â€” pick scene/strategy from a menu, see live heatmap.

---

## Architecture Reference

```
raymarching_benchmark/
â”œâ”€â”€ main.py                  # CLI entry point and orchestrator
â”œâ”€â”€ config.py                # RenderConfig, MarchConfig, BenchmarkConfig
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ vec3.py              # Vec3 + utilities
â”‚   â”œâ”€â”€ ray.py               # Ray(origin, direction)
â”‚   â”œâ”€â”€ camera.py            # Perspective camera, ray generation
â”‚   â””â”€â”€ types.py             # MarchResult (per-ray), RayMarchStats (aggregate)
â”œâ”€â”€ scenes/
â”‚   â”œâ”€â”€ base.py              # Abstract SDFScene
â”‚   â”œâ”€â”€ primitives.py        # SDF functions + combinators
â”‚   â””â”€â”€ catalog.py           # 14 test scenes
â”œâ”€â”€ strategies/
â”‚   â”œâ”€â”€ base.py              # Abstract MarchStrategy
â”‚   â”œâ”€â”€ standard_sphere.py   # Standard sphere tracing
â”‚   â”œâ”€â”€ overstep_bisect.py   # Overstep + bisection
â”‚   â”œâ”€â”€ adaptive_hybrid.py   # Adaptive hybrid (mode switching)
â”‚   â”œâ”€â”€ relaxed_sphere.py    # Fixed-omega relaxed
â”‚   â”œâ”€â”€ auto_relaxed.py      # Auto-relaxed (AR-ST, EMA-based omega)
â”‚   â”œâ”€â”€ enhanced_sphere.py   # Enhanced (planar extrapolation)
â”‚   â””â”€â”€ segment_tracing.py   # Segment tracing (Lipschitz bounds)
â”œâ”€â”€ metrics/
â”‚   â”œâ”€â”€ collector.py         # MetricsCollector â€” run rays, time, compute stats
â”‚   â””â”€â”€ analyzer.py          # MetricsAnalyzer â€” cross-strategy comparison
â””â”€â”€ visualization/
    â”œâ”€â”€ heatmaps.py          # Iteration/hit/depth heatmap PNGs
    â”œâ”€â”€ tables.py            # Console comparison tables
    â””â”€â”€ charts.py            # Comparative charts (stub)
```
